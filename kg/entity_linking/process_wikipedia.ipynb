{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "least-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "import mwparserfromhell\n",
    "\n",
    "from kg.entity_linking import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pending-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dir = '/Users/tmorrill002/Documents/datasets/wikipedia/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "separate-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_dir = '/Users/tmorrill002/Documents/datasets/wikipedia/20210401/'\n",
    "file = 'enwiki-20210401-pages-articles-multistream1.xml-p1p41242.bz2'\n",
    "\n",
    "file_path = os.path.join(wiki_dir, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-romance",
   "metadata": {},
   "source": [
    "**Overview of the parsing process:**\n",
    "1) etree.iterparse incrementally builds up the XML tree one line at a time\n",
    "2) the root node stores children nodes underneath it, which correspond to wikipedia pages\n",
    "3) the 'start' event corresponds to an opening tag (e.g. \\<page\\>) and the 'end' event corresponds to a closing tag (e.g. \\</page\\>)\n",
    "4) the elem.text method gathers all text between the start and end of the element, which is built up incrementally as more lines are parsed\n",
    "5) the root is cleared once a page is parsed so that there is only one child node under the root at a time (keeps memory footprint low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(file_path):\n",
    "    # open the compressed file\n",
    "    fp = bz2.BZ2File(file_path, 'r')\n",
    "\n",
    "    # handles first element in the iteration, which is later cleared to avoid persisting everything in memory\n",
    "    # get an iterable\n",
    "    context = etree.iterparse(fp, events=(\"start\", \"end\"))\n",
    "\n",
    "    # turn it into an iterator\n",
    "    context = iter(context)\n",
    "\n",
    "    # get the root element\n",
    "    event, root = next(context)\n",
    "    return context, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defensive-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(context, root):\n",
    "    extracted_content = {}\n",
    "    for event, elem in context:\n",
    "        # clean tags: '{http://www.mediawiki.org/xml/export-0.10/}mediawiki' -> 'mediawiki'\n",
    "        tag = utils.strip_tag_name(elem.tag)\n",
    "        \n",
    "        # capture information when the close tag is read (e.g. <\\page>)\n",
    "        if event == 'end':\n",
    "            # get text content that has been accumulated\n",
    "            if tag == 'title':\n",
    "                extracted_content['title'] = elem.text\n",
    "            elif tag == 'text':\n",
    "                extracted_content['text'] = elem.text\n",
    "            elif tag == 'id':\n",
    "                extracted_content['id'] = int(elem.text)\n",
    "            elif tag == 'redirect':\n",
    "                extracted_content['redirect'] = elem.attrib['title']\n",
    "            elif tag == 'ns':\n",
    "                extracted_content['ns'] = int(elem.text)\n",
    "            # read one complete page\n",
    "            elif tag == 'page':\n",
    "                # keep memory footprint low, clear all the children nodes\n",
    "                root.clear()\n",
    "                \n",
    "                return extracted_content\n",
    "    \n",
    "    # if nothing left to iterate on, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "satisfactory-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, root = get_context(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "stone-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 0\n",
    "page = True\n",
    "while page:\n",
    "    page = get_page(context, root)\n",
    "    page_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "integral-jewelry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27435"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bound-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this \n",
    "for child in root:\n",
    "    print({x.tag for x in root.findall(child.tag+\"/*\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-divide",
   "metadata": {},
   "source": [
    "**Link extraction specification:**\n",
    "1. Dictionary mapping surface forms (i.e. anchor text) to entities\n",
    "    - Key should be anchor text, value should be dictionary, where keys are entities and values are mention counts\n",
    "1. Article titles should always make it into the dictionary\n",
    "    - this is important when no one links to a page\n",
    "    - may want to do some additional cleanup on these (e.g. remove parentheses)\n",
    "1. Backburner: inverse this dictionary and map entities to surface forms, if needed\n",
    "\n",
    "Refer to [this post](https://ai.googleblog.com/2012/05/from-words-to-concepts-and-back.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-sheet",
   "metadata": {},
   "source": [
    "Issues to solve for:\n",
    "1. Extracted text doesn't start and end with 2 open/close brackets\n",
    "1. More than 1 pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "chinese-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_anchor_pairs(page):\n",
    "    \"\"\"Get pairs of entities and anchors from a page.\"\"\"\n",
    "    # create the wiki article\n",
    "    # grateful for this library!!\n",
    "    wiki_page = mwparserfromhell.parse(page['text'])\n",
    "\n",
    "    # find the wikilinks\n",
    "    wikilinks = [x for x in wiki.filter_wikilinks()]\n",
    "\n",
    "    entity_anchor_pairs = []\n",
    "    issues = []\n",
    "    for link in wikilinks:\n",
    "        # links look like this: '[[Political movement|movement]]'\n",
    "        # the first part is the entity (link to another wiki page)\n",
    "        # the second is the anchor text (i.e. surface form)\n",
    "        \n",
    "        if link[:2] != '[[' or link[-2:] != ']]':\n",
    "            # issue if doens't start with '[[' and end with ']]'\n",
    "            issues.append(link)\n",
    "            continue\n",
    "\n",
    "        trimmed_link = link[2:-2]\n",
    "        link_parts = trimmed_link.split('|')\n",
    "        if len(link_parts) > 2:\n",
    "            # issue if more than 1 pipe\n",
    "            issues.append(link)\n",
    "            continue\n",
    "        \n",
    "        if len(link_parts) == 1:\n",
    "            # possible that the link is related to a category e.g. 'Category:Anti-capitalism'\n",
    "            if link_parts[0].startswith('Category:'):\n",
    "                # TODO: determine if we want to handle categories differently\n",
    "                # strip category tag from the surface form\n",
    "                clean_category = link_parts[0].split('Category:')[-1]\n",
    "                # e.g. entity, anchor = 'Category:Anti-capitalism', 'Anti-capitalism'\n",
    "                entity, anchor = link_parts[0], clean_category\n",
    "            else:\n",
    "                # i.e. anchor text is the same as the entity name\n",
    "                entity, anchor = link_parts[0], link_parts[0]\n",
    "        elif len(link_parts) == 2:\n",
    "            # expected format\n",
    "            entity, anchor = link_parts[0], link_parts[1]\n",
    "\n",
    "        entity_anchor_pairs.append((entity, anchor))\n",
    "    \n",
    "    return entity_anchor_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "advanced-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(page):\n",
    "    # anchor_text -> {entity: count, another_entity: count}\n",
    "    anchor_to_entities = defaultdict(Counter)\n",
    "    \n",
    "    # add redirects to dictionary (probably needs some string cleanup, mostly camel case)\n",
    "    if 'redirect' in page:\n",
    "        # e.g. page['title'] = 'AfghanistanHistory'\n",
    "        # page['redirect'] = 'History of Afghanistan'\n",
    "        anchor_text = page['title']\n",
    "        entity = page['redirect']\n",
    "        anchor_to_entities[anchor_text][entity] += 1\n",
    "        \n",
    "        # no other links on page, simply return\n",
    "        return anchor_to_entities\n",
    "    \n",
    "    # add the title to the dictionary\n",
    "    # useful if no other page links to this page\n",
    "    title = page['title']\n",
    "    anchor_to_entities[title][title] += 1\n",
    "    \n",
    "    # get links and update anchor, entity occurrence counts\n",
    "    # result will look like:\n",
    "    # 'Carl Levy': Counter({'Carl Levy (political scientist)': 1})\n",
    "    # 'capitalism': Counter({'Anarchism and capitalism': 2, 'capitalism': 1})\n",
    "    entity_anchor_pairs = get_entity_anchor_pairs(page)\n",
    "    for entity, anchor in entity_anchor_pairs:\n",
    "        anchor_to_entities[anchor][entity] += 1\n",
    "    \n",
    "    return anchor_to_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "military-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_to_entities_dict = extract_links(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "alpha-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anarchism and capitalism', 2), ('capitalism', 1)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_to_entities['capitalism'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-irish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Knowledge Graphs",
   "language": "python",
   "name": "knowledge-graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
