# mp_example.py Program Report

## Overview
It is worth proving the correctness of the behavior of mp_example.py simply because it is so easy to get wrong. Developing and debugging parallel programs is best done through a deeper understanding of the semantics of these programs, rather than the traditional breakpoint/print driven approach. Race conditions can be hard to identify. It can be challenging to write a parallel program that doesn't waste CPU cycles on polling tasks and maintains a manageable memory footprint. Furthermore, *this* program should never spend any time on undue waiting (e.g. using timeouts to wait for a queue item, etc.).

## The producer-consumer problem
The producer-consumer problem attempts to synchronize computation between two pools of workers through the use of queues. There are several motivations for this architecure, in my case. The first reason is that I would like to parallelize my computations as much as possible in a map-reduce type fashion in order to save time processing a lot of data. The second reason is that by limiting the queue size, I can control how much work is in-memory at any one given time. This allows me to process datasets that are far larger than the memory on my MacBook Pro (32Gb).

### The producer
The `producer` function in mp_example.py runs in a continuous `while True` loop reading data from the `input_queue` and in theory, would do something useful to the data (e.g. load it into memory, do some initial processing, etc.). The `input_queue` could be a list of file names or any data that needs to be processed. The `input_queue` should contain a sentinel values, in our case `None`,for all the producer processes, which will signal to the `producer` that it can safely exit and that it should expect no further data entering the `input_queue`. The `producer` will notify downstream `consumer`s that it has exited by adding `None` to the `output_queue`. This will become important later when determining if it is safe to shutdown the `consumer` processes.

If the `producer` reads anything other than the sentinel value from the `input_queue` it should do meaningful work and add it to the `output_queue`. The `output_queue` is the output from the `producer`'s perspective but the input from `consumer`'s perspective. The `output_queue` size is limited, which is a critical feature in this architecture. For example, suppose `producers` could add to the `output_queue` faster than `consumer`s could consume items. This would mean that the `output_queue` size would grow unabated and the machine would run out of memory before everything could be processed. Instead, if the `output_queue` is full according to its `maxsize`, the `producer` simply goes to sleep and waits until the `output_queue` has space. No computational cycles are wasted repeatedly trying to add an item to a queue that is full. Simultaneously, there are no concerns of running out of memory.

### The consumer
In a word, the `consumer` reads and processes data from the `output_queue`. These data points could be blobs of text to be processed or really any data requiring processing. What's interesting, is that we use a read/write protected variable called `producers_exited`, which keeps track of how many `producer`s have exited. The logic is simple. If all `producer`s have exited, then we can be 100% certain that nothing new will be added to the `output_queue`. Next, if the queue is empty, we can be confident that the `consumer` has nothing left to do but shutdown. There are two race conditions that must be addressed. The first race condition is when we are told that the queue is *not* empty. What could then happen is that we proceed to get something from the queue but another process removes the final item from the queue first. In this case, the `consumer` will wait forever for get something from the queue. This means deadlock for the process. To solve this, we need to be sure that only one process can access the queue at a time so we can be confident that if the `output_queue` is not empty, it will have an item for us to get with 100% certainty and we don't have to worry about deadlock. In this program, the queue access is synchronized using the variable `producers_exited`. The second race condition to address is the potential for multiple processes to read/write the `producers_exited` variable at the same time. Suppose two processes both read a value of 1 for `producers_exited` and then they both receive a `None` sentinel from the queue. They both attempt to increment the value of `producers_exited` by 1 and the net result is that `producers_exited.value` is 2 instead of the expected 3. The solution is to use a lock object to ensure that only one process reads and modifies this value at a time. This has the nice benefit of synchronizing access to the queue as well since  a process can only read from the queue if it has the `producers_exited` lock.

Another point worth noting is that the actual processing of any data from the queue must happen outside of the `with producers_exited.get_lock()` context. If the work took place inside the context, then the result would be serial processing of the data with only one process doing work at a time (due to the lock) - not the parallel processing we hoped for.

Finally, this process can be repeated in multiple stages as can be seen by the `final_queue`, which accepts the output from the consumers. From here, another set of "consumers" could ingest the work in the `final_queue`.

### Implementation notes
I used the same number of processes for `producer`s and `consumer`s and am trusting that limiting the size of the queue will balance the amount of time each pool of processes spends running. This could probably be tuned experimentally to maximize throughput of the system. You could vary the `maxsize` of the queue according the memory available on the machine, 
as well as vary the mix of `producer` and `consumer` processes and measure something like the number of items processed per minute.

During my implementation of this system, I [learned](https://stackoverflow.com/questions/56321756/multiprocessing-queue-with-hugh-data-causes-wait-for-tstate-lock) that once a process works with a queue, that queue must be fully exhausted (empty or closed) before that process will successfully join back to the main program and [allow the main program to terminate](https://docs.python.org/3.5/library/multiprocessing.html#pipes-and-queues). Be sure to exhaust all your queues before joining the processes.

The correctness of this program was proven above using the semantics of locks, queues, and processes. This can be experimentally verified by checking that all the elements that entered the producer-consumer system were returned as expected (see `assert expected == len(results)`).

