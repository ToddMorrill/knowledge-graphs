{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import kg.ner.utils as utils\n",
    "from kg.ner.unsupervised import NounPhraseDetection, EntityDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/Users/tmorrill002/Documents/datasets/conll/transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = utils.load_train_data(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What fraction of NER tags are Noun Phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_dict['train.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_over_NER(df, noun_col = 'Chunk_Tag', ner_col = 'NER_Tag_Normalized'):\n",
    "    ner_df = df[df[ner_col] != 'O']\n",
    "    noun_phrase_token_count = len(ner_df[(ner_df[noun_col] == 'I-NP') | (ner_df[noun_col] == 'B-NP')])\n",
    "    print(f'Count of noun phrase tokens among NER tokens: {noun_phrase_token_count}')\n",
    "    print(f'Count of NER tokens: {len(ner_df)}')\n",
    "    print(f'Percent of NER tokens that are part of noun phrases: {round(noun_phrase_token_count / len(ner_df),4) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of noun phrase tokens among NER tokens: 33054\n",
      "Count of NER tokens: 34043\n",
      "Percent of NER tokens that are part of noun phrases: 97.09%\n"
     ]
    }
   ],
   "source": [
    "nouns_over_NER(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What fraction of Noun Phrase tokens are NER tagged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER_over_nouns(df, noun_col = 'Chunk_Tag', ner_col = 'NER_Tag_Normalized'):\n",
    "    noun_phrase_df = df[(df[noun_col] == 'I-NP') | (df[noun_col] == 'B-NP')]\n",
    "    ner_tag_token_count = len(noun_phrase_df[noun_phrase_df[ner_col] != 'O'])\n",
    "    print(f'Count of NER tokens among noun phrase tokens: {ner_tag_token_count}')\n",
    "    print(f'Count of noun phrase tokens: {len(noun_phrase_df)}')\n",
    "    print(f'Percent of noun phrase tokens that are part of NER tags: {round(ner_tag_token_count / len(noun_phrase_df), 4) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of NER tokens among noun phrase tokens: 33054\n",
      "Count of noun phrase tokens: 124032\n",
      "Percent of noun phrase tokens that are part of NER tags: 26.650000000000002%\n"
     ]
    }
   ],
   "source": [
    "NER_over_nouns(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "1. NER Tags are almost exclusively noun phrases (97%) -> noun phrase candidates will yield high recall\n",
    "2. Noun phrases encompass a lot more than NER tags -> noun phrase candidates will yield low precision and other techniques should be used to reduce the number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Unsupervised Noun Phrase Detection Against CoNLL-2000 and CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_2000_test_sentences = conll2000.chunked_sents('test.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = NounPhraseDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "print(chunk_parser.evaluate(conll_2000_test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(x):\n",
    "    return (x['Token'], x['POS_Tag'], x['Chunk_Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tags'] = train_df.apply(consolidate, axis=1) #passes a Series object, row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth labeled data containing (token, pos, chunk), used for evaluation\n",
    "train_conll_2003 = train_df.groupby(['Article_ID', 'Sentence_ID'], )['Tags'].apply(list).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagged sentences only containing (token, pos), used to make predictions\n",
    "train_conll_2003_pos_tags = []\n",
    "for sentence in train_conll_2003:\n",
    "    temp_sentence = []\n",
    "    for token, pos, chunk in sentence:\n",
    "        temp_sentence.append((token, pos))\n",
    "    train_conll_2003_pos_tags.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples must be in \"tree\" format to use evaluation code\n",
    "trees = []\n",
    "for example in train_conll_2003:\n",
    "    trees.append(nltk.chunk.conlltags2tree(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  67.4%%\n",
      "    Precision:     80.5%%\n",
      "    Recall:        43.7%%\n",
      "    F-Measure:     56.7%%\n"
     ]
    }
   ],
   "source": [
    "# getting harmed here because B-NP tags aren't marked appropriately in CoNLL-2003\n",
    "print(chunk_parser.evaluate(trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_trees = []\n",
    "for example in train_conll_2003_pos_tags:\n",
    "    prediction_trees.append(chunk_parser.parse(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in prediction_trees:\n",
    "    predictions.append(nltk.chunk.tree2conlltags(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = [prediction for sentence in predictions for prediction in sentence ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Prediction'] = flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Prediction'] = train_df['Prediction'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Noun_Phrase'] = train_df['Prediction'] != 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of noun phrase tokens among NER tokens: 33030\n",
      "Count of NER tokens: 34043\n",
      "Percent of NER tokens that are part of noun phrases: 97.02%\n"
     ]
    }
   ],
   "source": [
    "nouns_over_NER(train_df, 'Prediction', 'NER_Tag_Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of NER tokens among noun phrase tokens: 33030\n",
      "Count of noun phrase tokens: 124428\n",
      "Percent of noun phrase tokens that are part of NER tags: 26.55%\n"
     ]
    }
   ],
   "source": [
    "NER_over_nouns(train_df, 'Prediction', 'NER_Tag_Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['NP_Chunk_Tag'] = (train_df['Chunk_Tag'] == 'I-NP') | (train_df['Chunk_Tag'] == 'B-NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9570476522559068"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agreement between existing ConLL-2003 noun phrase tags and the predicted tags\n",
    "(train_df['Noun_Phrase'] == train_df['NP_Chunk_Tag']).sum() / len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TF-IDF Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extractor = EntityDetection(chunk_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['NER_Tag_Flag'] = train_df['NER_Tag'] != 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up articles\n",
    "articles = train_df.groupby(['Article_ID'], )['Token'].apply(lambda x: ' '.join([str(y) for y in list(x)])).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extractor.fit_tfidf(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = article.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "for article in articles:\n",
    "    # manually tokenize because nltk tokenizer is converting 'C$' -> ['C', '$'] and throwing off comparison\n",
    "    sentences = utils.tokenize_text(article)\n",
    "    article = [sentence.split() for sentence in sentences]\n",
    "    article = utils.tag_pos(article)\n",
    "    for candidate in entity_extractor.candidates(article, preprocess = False):\n",
    "        candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmorrill002/Documents/knowledge-graphs/kg/ner/unsupervised.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  scores = sums / token_counts\n"
     ]
    }
   ],
   "source": [
    "scores = entity_extractor.score_phrases(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(scores, columns=['Phrase', 'Noun_Phrase_Flag', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df['Phrase_ID'] = prediction_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Noun_Phrase_Flag</th>\n",
       "      <th>Score</th>\n",
       "      <th>Phrase_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU</td>\n",
       "      <td>True</td>\n",
       "      <td>5.455404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rejects</td>\n",
       "      <td>False</td>\n",
       "      <td>7.160152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German call</td>\n",
       "      <td>True</td>\n",
       "      <td>4.048864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to boycott</td>\n",
       "      <td>False</td>\n",
       "      <td>3.813180</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>British lamb</td>\n",
       "      <td>True</td>\n",
       "      <td>5.459553</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Phrase  Noun_Phrase_Flag     Score  Phrase_ID\n",
       "0            EU              True  5.455404          0\n",
       "1       rejects             False  7.160152          1\n",
       "2   German call              True  4.048864          2\n",
       "3    to boycott             False  3.813180          3\n",
       "4  British lamb              True  5.459553          4"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df['Phrase'] = prediction_df['Phrase'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = prediction_df.explode('Phrase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation isn't getting assigned a score, fill with zero for now\n",
    "prediction_df['Score'] = prediction_df['Score'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = prediction_df.rename(columns={'Phrase': 'Predicted_Phrase'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate why this is happening\n",
    "prediction_df['Predicted_Phrase'] = prediction_df['Predicted_Phrase'].replace('``', '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = train_df[train_df['Article_ID'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.concat((train_df, prediction_df.reset_index(drop=True)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_df) == len(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why are some CoNLL-2003 tokens NaN?\n",
    "eval_df = eval_df.dropna(subset=['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (eval_df['Token'] == eval_df['Predicted_Phrase']).sum() == len(eval_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-bc9f9be2371e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['Predicted_Entity_Flag'] = eval_df['Noun_Phrase_Flag']\n"
     ]
    }
   ],
   "source": [
    "eval_df['Predicted_Entity_Flag'] = eval_df['Noun_Phrase_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.51      0.68    169575\n",
      "        True       0.29      0.98      0.44     34043\n",
      "\n",
      "    accuracy                           0.59    203618\n",
      "   macro avg       0.64      0.74      0.56    203618\n",
      "weighted avg       0.87      0.59      0.64    203618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline of just using noun phrases to identify entities (high recall, low precision)\n",
    "print(classification_report(eval_df['NER_Tag_Flag'], eval_df['Predicted_Entity_Flag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_threshold = eval_df['Score'].describe()['50%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-153-6c5eb2a035e3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['Predicted_Entity_Flag_TFIDF_Median'] = (eval_df['Noun_Phrase_Flag'] & (eval_df['Score'] > median_threshold))\n"
     ]
    }
   ],
   "source": [
    "# use TF-IDF score\n",
    "eval_df['Predicted_Entity_Flag_TFIDF_Median'] = (eval_df['Noun_Phrase_Flag'] & (eval_df['Score'] > median_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.69      0.80    169575\n",
      "        True       0.34      0.81      0.48     34043\n",
      "\n",
      "    accuracy                           0.71    203618\n",
      "   macro avg       0.64      0.75      0.64    203618\n",
      "weighted avg       0.85      0.71      0.74    203618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using noun phrases and median TFIDF score to identify entities\n",
    "print(classification_report(eval_df['NER_Tag_Flag'], eval_df['Predicted_Entity_Flag_TFIDF_Median']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize threshold to maximize macro f1\n",
    "start = eval_df['Score'].describe()['min']\n",
    "stop = eval_df['Score'].describe()['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.25 increments\n",
    "predictions = []\n",
    "for thresh in np.arange(start, stop, step=0.25):\n",
    "    predictions.append((thresh, (eval_df['Noun_Phrase_Flag'] & (eval_df['Score'] > thresh)).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1_scores = []\n",
    "for prediction in predictions:\n",
    "    report = classification_report(eval_df['NER_Tag_Flag'], prediction[1], output_dict=True)\n",
    "    macro_f1_scores.append((prediction[0], report['True']['f1-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.5, 0.4838175288794013)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(macro_f1_scores, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.80      0.85    169575\n",
      "        True       0.39      0.64      0.48     34043\n",
      "\n",
      "    accuracy                           0.77    203618\n",
      "   macro avg       0.65      0.72      0.67    203618\n",
      "weighted avg       0.83      0.77      0.79    203618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(eval_df['NER_Tag_Flag'], (eval_df['Noun_Phrase_Flag'] & (eval_df['Score'] > 4.5))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - Knowledge Graphs",
   "language": "python",
   "name": "knowledge-graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
