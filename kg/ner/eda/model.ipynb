{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "from kg.ner.model import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('/Users/tmorrill002/Documents/datasets/conll/transformed/train.csv')\n",
    "val_df = pd.read_csv('/Users/tmorrill002/Documents/datasets/conll/transformed/validation.csv')\n",
    "test_df = pd.read_csv('/Users/tmorrill002/Documents/datasets/conll/transformed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary and label dictionaries\n",
    "vocab = torchtext.vocab.Vocab(Counter(train_df['Token'].value_counts().to_dict()))\n",
    "label_dict = {}\n",
    "i = 0\n",
    "for k in train_df['NER_Tag_Normalized'].unique():\n",
    "    label_dict[k] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLL2003Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, vocab, label_dict, transform=None):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "        self.sentences, self.labels = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        temp_df = self.df.groupby(['Article_ID', 'Sentence_ID'], as_index=False).agg(Sentence=('Token', list), Labels=('NER_Tag_Normalized', list))\n",
    "        sentences = temp_df['Sentence'].values.tolist()\n",
    "        labels = temp_df['Labels'].values.tolist()\n",
    "        return sentences, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        indices = []\n",
    "        for token in self.sentences[idx]:\n",
    "            indices.append(self.vocab[token])\n",
    "        labels = []\n",
    "        for label in self.labels[idx]:\n",
    "            labels.append(self.label_dict[label])\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoNLL2003Dataset(train_df, vocab, label_dict)\n",
    "val_dataset = CoNLL2003Dataset(val_df, vocab, label_dict)\n",
    "test_dataset = CoNLL2003Dataset(test_df, vocab, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vocab[train_df.iloc[0]['Token']] == train_dataset[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert label_dict[train_df.iloc[0]['NER_Tag_Normalized']] == train_dataset[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-19a9c619550e>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_batch.append(torch.tensor(train_dataset[0][0]))\n",
      "<ipython-input-8-19a9c619550e>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_batch.append(torch.tensor(train_dataset[1][0]))\n",
      "<ipython-input-8-19a9c619550e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_labels_batch.append(torch.tensor(train_dataset[0][1]))\n",
      "<ipython-input-8-19a9c619550e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_labels_batch.append(torch.tensor(train_dataset[1][1]))\n"
     ]
    }
   ],
   "source": [
    "small_batch = []\n",
    "small_batch.append(torch.tensor(train_dataset[0][0]))\n",
    "small_batch.append(torch.tensor(train_dataset[1][0]))\n",
    "small_batch_lens = [len(x) for x in small_batch]\n",
    "\n",
    "small_labels_batch = []\n",
    "small_labels_batch.append(torch.tensor(train_dataset[0][1]))\n",
    "small_labels_batch.append(torch.tensor(train_dataset[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch_padded = pad_sequence(small_batch, batch_first=True, padding_value=vocab['<pad>'])\n",
    "small_labels_batch_padded = pad_sequence(small_labels_batch, batch_first=True, padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  964, 22406,   236,   771,     7,  4586,   210,  7683,     2],\n",
       "        [  737,  2088,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  1,  1,  1,  2,  1,  1],\n",
       "        [ 3,  3, -1, -1, -1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_labels_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(small_batch_padded, small_batch_lens, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([  964,   737, 22406,  2088,   236,   771,     7,  4586,   210,  7683,\n",
       "            2]), batch_sizes=tensor([2, 2, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, sequence_lengths = pad_packed_sequence(packed, batch_first=True, padding_value=vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_size': 128,\n",
    "    'num_classes': len(label_dict),\n",
    "    'batch_size': 16\n",
    "}\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model((small_batch_padded, small_batch_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    labels = labels.reshape(-1)\n",
    "    mask = (labels >= 0).float()\n",
    "    labels = labels % outputs.shape[1]\n",
    "    num_tokens = mask.sum()\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels] * mask) / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5694, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(output, small_labels_batch_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sentence_indices, sentence_labels = zip(*batch)\n",
    "    sentence_lens = [len(x) for x in sentence_indices]\n",
    "    \n",
    "    # vocab['<pad>'] = 1\n",
    "    sentences_padded = pad_sequence(sentence_indices, batch_first=True, padding_value=1)\n",
    "    labels_padded = pad_sequence(sentence_labels, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return (sentences_padded, sentence_lens), labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(output, lengths, concatenate=True):\n",
    "    # extract predictions\n",
    "    max_len = max(lengths)\n",
    "    preds = output.argmax(dim=1)\n",
    "    i = 0\n",
    "    preds_list = []\n",
    "    for length in lengths:\n",
    "        start = i*max_len\n",
    "        stop = start + length\n",
    "        preds_list.append(preds[start:stop])\n",
    "        i += 1\n",
    "    if concatenate:\n",
    "        return torch.cat(preds_list)\n",
    "    return preds_list\n",
    "\n",
    "def recover_labels(padded_labels, lengths):\n",
    "    # extract labels\n",
    "    max_len = max(lengths)\n",
    "    labels_vector = padded_labels.reshape(-1)\n",
    "    i = 0\n",
    "    labels_list = []\n",
    "    for length in lengths:\n",
    "        start = i*max_len\n",
    "        stop = start + length\n",
    "        labels_list.append(labels_vector[start:stop])\n",
    "        i += 1\n",
    "    return torch.cat(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, sentences, labels, dataset='Train'):\n",
    "    batch_preds = get_predictions(output, sentences[1])\n",
    "    batch_labels = recover_labels(labels, sentences[1])\n",
    "    raw_acc = accuracy_score(batch_preds, batch_labels)\n",
    "    acc = round(raw_acc * 100, 2)\n",
    "    # print(f'{dataset} accuracy score: {acc}%')\n",
    "    return raw_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "Sample train batch - loss value: 1.5594 \t accuracy score: 37.89%\n",
      "Sample train batch - loss value: 0.5848 \t accuracy score: 82.83%\n",
      "Sample train batch - loss value: 0.656 \t accuracy score: 77.13%\n",
      "Sample train batch - loss value: 0.3748 \t accuracy score: 88.73%\n",
      "Sample train batch - loss value: 0.4269 \t accuracy score: 85.38%\n",
      "Sample train batch - loss value: 0.3668 \t accuracy score: 88.5%\n",
      "Sample train batch - loss value: 0.2897 \t accuracy score: 90.36%\n",
      "Sample train batch - loss value: 0.3677 \t accuracy score: 86.23%\n",
      "Sample train batch - loss value: 0.2 \t accuracy score: 93.51%\n",
      "Average validation loss: 0.3073\n",
      "Validation accuracy score: 90.1%\n",
      "\n",
      "Epoch number: 2\n",
      "Sample train batch - loss value: 0.2267 \t accuracy score: 92.89%\n",
      "Sample train batch - loss value: 0.3443 \t accuracy score: 91.11%\n",
      "Sample train batch - loss value: 0.165 \t accuracy score: 95.11%\n",
      "Sample train batch - loss value: 0.1788 \t accuracy score: 94.77%\n",
      "Sample train batch - loss value: 0.1946 \t accuracy score: 93.68%\n",
      "Sample train batch - loss value: 0.3267 \t accuracy score: 91.48%\n",
      "Sample train batch - loss value: 0.2068 \t accuracy score: 91.62%\n",
      "Sample train batch - loss value: 0.2042 \t accuracy score: 94.4%\n",
      "Sample train batch - loss value: 0.3067 \t accuracy score: 89.22%\n",
      "Average validation loss: 0.2087\n",
      "Validation accuracy score: 93.36%\n",
      "\n",
      "Epoch number: 3\n",
      "Sample train batch - loss value: 0.0859 \t accuracy score: 97.93%\n",
      "Sample train batch - loss value: 0.0557 \t accuracy score: 99.24%\n",
      "Sample train batch - loss value: 0.1059 \t accuracy score: 96.53%\n",
      "Sample train batch - loss value: 0.1122 \t accuracy score: 97.28%\n",
      "Sample train batch - loss value: 0.1965 \t accuracy score: 94.3%\n",
      "Sample train batch - loss value: 0.0876 \t accuracy score: 97.42%\n",
      "Sample train batch - loss value: 0.1084 \t accuracy score: 96.88%\n",
      "Sample train batch - loss value: 0.0917 \t accuracy score: 96.91%\n",
      "Sample train batch - loss value: 0.0918 \t accuracy score: 97.36%\n",
      "Average validation loss: 0.1776\n",
      "Validation accuracy score: 94.61%\n",
      "\n",
      "Epoch number: 4\n",
      "Sample train batch - loss value: 0.0501 \t accuracy score: 98.28%\n",
      "Sample train batch - loss value: 0.0857 \t accuracy score: 97.62%\n",
      "Sample train batch - loss value: 0.0304 \t accuracy score: 99.65%\n",
      "Sample train batch - loss value: 0.0828 \t accuracy score: 96.9%\n",
      "Sample train batch - loss value: 0.0458 \t accuracy score: 98.85%\n",
      "Sample train batch - loss value: 0.0436 \t accuracy score: 99.0%\n",
      "Sample train batch - loss value: 0.1251 \t accuracy score: 96.3%\n",
      "Sample train batch - loss value: 0.0497 \t accuracy score: 98.73%\n",
      "Sample train batch - loss value: 0.025 \t accuracy score: 99.08%\n",
      "Average validation loss: 0.173\n",
      "Validation accuracy score: 94.84%\n",
      "\n",
      "Epoch number: 5\n",
      "Sample train batch - loss value: 0.0306 \t accuracy score: 99.51%\n",
      "Sample train batch - loss value: 0.0307 \t accuracy score: 99.39%\n",
      "Sample train batch - loss value: 0.0231 \t accuracy score: 98.85%\n",
      "Sample train batch - loss value: 0.0266 \t accuracy score: 99.26%\n",
      "Sample train batch - loss value: 0.0354 \t accuracy score: 99.27%\n",
      "Sample train batch - loss value: 0.0253 \t accuracy score: 99.51%\n",
      "Sample train batch - loss value: 0.0264 \t accuracy score: 99.32%\n",
      "Sample train batch - loss value: 0.0618 \t accuracy score: 97.59%\n",
      "Sample train batch - loss value: 0.0233 \t accuracy score: 99.56%\n",
      "Average validation loss: 0.1828\n",
      "Validation accuracy score: 94.85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATIENCE=3\n",
    "running_patience = PATIENCE\n",
    "best_val_loss = np.inf\n",
    "record_loss = []\n",
    "for i in range(5):\n",
    "    model.train()\n",
    "    print(f'Epoch number: {i+1}')\n",
    "    j = 0\n",
    "    for sentences, labels in train_dataloader:\n",
    "        output = model(sentences)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 100 == 0:\n",
    "            record_loss.append(loss)\n",
    "            print(f'Sample train batch - loss value: {round(loss.item(), 4)} \\t accuracy score: {round(metrics(output, sentences, labels)*100, 2)}%')\n",
    "        j += 1\n",
    "    \n",
    "    # monitor validation loss\n",
    "    model.eval()\n",
    "    val_loss_scores = []\n",
    "    val_acc_scores = []\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for sentences, labels in val_dataloader:\n",
    "        output = model(sentences)\n",
    "        loss = loss_fn(output, labels)\n",
    "        val_loss_scores.append(loss.item())\n",
    "        val_acc_scores.append(metrics(output, sentences, labels))\n",
    "        val_preds += get_predictions(output, sentences[1]).tolist()\n",
    "        val_labels += recover_labels(labels, sentences[1]).tolist()\n",
    "    val_loss = round(np.mean(val_loss_scores), 4)\n",
    "    val_acc = round(np.mean(val_acc_scores)*100, 2)\n",
    "    print(f'Average validation loss: {val_loss}')\n",
    "    print(f'Validation accuracy score: {val_acc}%')\n",
    "    \n",
    "    # stopping criterion\n",
    "    if val_loss < best_val_loss:\n",
    "        running_patience = PATIENCE\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        running_patience -= 1\n",
    "        if running_patience == 0:\n",
    "            print(f'Model has not improved for {PATIENCE} epochs. Stopping training.')\n",
    "            break\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG       0.83      0.64      0.72      2092\n",
      "           O       0.97      0.99      0.98     42759\n",
      "        MISC       0.88      0.70      0.78      1268\n",
      "         PER       0.78      0.85      0.82      3149\n",
      "         LOC       0.85      0.82      0.84      2094\n",
      "\n",
      "    accuracy                           0.95     51362\n",
      "   macro avg       0.86      0.80      0.83     51362\n",
      "weighted avg       0.95      0.95      0.95     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_set = list(set(val_preds).union(set(val_labels)))\n",
    "\n",
    "print(classification_report(val_labels, val_preds, labels=label_set, target_names=list(label_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 0.2872\n",
      "Validation accuracy score: 91.39%\n"
     ]
    }
   ],
   "source": [
    "# monitor validation loss\n",
    "model.eval()\n",
    "val_loss_scores = []\n",
    "val_acc_scores = []\n",
    "val_preds = []\n",
    "val_labels = []\n",
    "for sentences, labels in test_dataloader:\n",
    "    output = model(sentences)\n",
    "    loss = loss_fn(output, labels)\n",
    "    val_loss_scores.append(loss.item())\n",
    "    val_acc_scores.append(metrics(output, sentences, labels))\n",
    "    val_preds += get_predictions(output, sentences[1]).tolist()\n",
    "    val_labels += recover_labels(labels, sentences[1]).tolist()\n",
    "val_loss = round(np.mean(val_loss_scores), 4)\n",
    "val_acc = round(np.mean(val_acc_scores)*100, 2)\n",
    "print(f'Average validation loss: {val_loss}')\n",
    "print(f'Validation accuracy score: {val_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG       0.84      0.56      0.67      2496\n",
      "           O       0.96      0.98      0.97     38323\n",
      "        MISC       0.75      0.61      0.68       918\n",
      "         PER       0.67      0.77      0.72      2773\n",
      "         LOC       0.79      0.78      0.78      1925\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.80      0.74      0.76     46435\n",
      "weighted avg       0.93      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_set = list(set(val_preds).union(set(val_labels)))\n",
    "\n",
    "print(classification_report(val_labels, val_preds, labels=label_set, target_names=list(label_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - Knowledge Graphs",
   "language": "python",
   "name": "knowledge-graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
