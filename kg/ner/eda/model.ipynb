{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "from kg.ner.model import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/tmorrill002/Documents/datasets/conll/transformed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.Vocab(Counter(train_df['Token'].value_counts().to_dict()))\n",
    "label_dict = {}\n",
    "i = 0\n",
    "for k in train_df['NER_Tag_Normalized'].unique():\n",
    "    label_dict[k] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLL2003Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, vocab, label_dict, transform=None):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.label_dict = label_dict\n",
    "        self.transform = transform\n",
    "        self.sentences, self.labels = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        temp_df = self.df.groupby(['Article_ID', 'Sentence_ID'], as_index=False).agg(Sentence=('Token', list), Labels=('NER_Tag_Normalized', list))\n",
    "        sentences = temp_df['Sentence'].values.tolist()\n",
    "        labels = temp_df['Labels'].values.tolist()\n",
    "        return sentences, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        indices = []\n",
    "        for token in self.sentences[idx]:\n",
    "            indices.append(self.vocab[token])\n",
    "        labels = []\n",
    "        for label in self.labels[idx]:\n",
    "            labels.append(self.label_dict[label])\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoNLL2003Dataset(train_df, vocab, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vocab[train_df.iloc[0]['Token']] == train_dataset[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert label_dict[train_df.iloc[0]['NER_Tag_Normalized']] == train_dataset[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  964, 22406,   236,   771,     7,  4586,   210,  7683,     2]),\n",
       " tensor([0, 1, 2, 1, 1, 1, 2, 1, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-19a9c619550e>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_batch.append(torch.tensor(train_dataset[0][0]))\n",
      "<ipython-input-9-19a9c619550e>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_batch.append(torch.tensor(train_dataset[1][0]))\n",
      "<ipython-input-9-19a9c619550e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_labels_batch.append(torch.tensor(train_dataset[0][1]))\n",
      "<ipython-input-9-19a9c619550e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  small_labels_batch.append(torch.tensor(train_dataset[1][1]))\n"
     ]
    }
   ],
   "source": [
    "small_batch = []\n",
    "small_batch.append(torch.tensor(train_dataset[0][0]))\n",
    "small_batch.append(torch.tensor(train_dataset[1][0]))\n",
    "small_batch_lens = [len(x) for x in small_batch]\n",
    "\n",
    "small_labels_batch = []\n",
    "small_labels_batch.append(torch.tensor(train_dataset[0][1]))\n",
    "small_labels_batch.append(torch.tensor(train_dataset[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch_padded = pad_sequence(small_batch, batch_first=True, padding_value=vocab['<pad>'])\n",
    "small_labels_batch_padded = pad_sequence(small_labels_batch, batch_first=True, padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  964, 22406,   236,   771,     7,  4586,   210,  7683,     2],\n",
       "        [  737,  2088,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  1,  1,  1,  2,  1,  1],\n",
       "        [ 3,  3, -1, -1, -1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_labels_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(small_batch_padded, small_batch_lens, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([  964,   737, 22406,  2088,   236,   771,     7,  4586,   210,  7683,\n",
       "            2]), batch_sizes=tensor([2, 2, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, sequence_lengths = pad_packed_sequence(packed, batch_first=True, padding_value=vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  964, 22406,   236,   771,     7,  4586,   210,  7683,     2],\n",
       "        [  737,  2088,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_size': 128,\n",
    "    'num_classes': len(label_dict),\n",
    "    'batch_size': 16\n",
    "}\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model((small_batch_padded, small_batch_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    labels = labels.reshape(-1)\n",
    "    mask = (labels >= 0).float()\n",
    "    labels = labels % outputs.shape[1]\n",
    "    num_tokens = mask.sum()\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels] * mask) / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6088, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(output, small_labels_batch_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sentence_indices, sentence_labels = zip(*batch)\n",
    "    sentence_lens = [len(x) for x in sentence_indices]\n",
    "    \n",
    "    # vocab['<pad>'] = 1\n",
    "    sentences_padded = pad_sequence(sentence_indices, batch_first=True, padding_value=1)\n",
    "    labels_padded = pad_sequence(sentence_labels, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return (sentences_padded, sentence_lens), labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "tensor(1.5981, grad_fn=<DivBackward0>)\n",
      "tensor(0.4196, grad_fn=<DivBackward0>)\n",
      "tensor(0.4185, grad_fn=<DivBackward0>)\n",
      "tensor(0.7137, grad_fn=<DivBackward0>)\n",
      "tensor(0.3584, grad_fn=<DivBackward0>)\n",
      "tensor(0.5985, grad_fn=<DivBackward0>)\n",
      "tensor(0.5766, grad_fn=<DivBackward0>)\n",
      "tensor(0.4007, grad_fn=<DivBackward0>)\n",
      "tensor(0.4434, grad_fn=<DivBackward0>)\n",
      "Epoch number: 2\n",
      "tensor(0.2937, grad_fn=<DivBackward0>)\n",
      "tensor(0.2776, grad_fn=<DivBackward0>)\n",
      "tensor(0.2724, grad_fn=<DivBackward0>)\n",
      "tensor(0.2294, grad_fn=<DivBackward0>)\n",
      "tensor(0.1081, grad_fn=<DivBackward0>)\n",
      "tensor(0.4313, grad_fn=<DivBackward0>)\n",
      "tensor(0.4322, grad_fn=<DivBackward0>)\n",
      "tensor(0.2291, grad_fn=<DivBackward0>)\n",
      "tensor(0.2432, grad_fn=<DivBackward0>)\n",
      "Epoch number: 3\n",
      "tensor(0.1745, grad_fn=<DivBackward0>)\n",
      "tensor(0.2005, grad_fn=<DivBackward0>)\n",
      "tensor(0.1723, grad_fn=<DivBackward0>)\n",
      "tensor(0.0786, grad_fn=<DivBackward0>)\n",
      "tensor(0.0304, grad_fn=<DivBackward0>)\n",
      "tensor(0.2829, grad_fn=<DivBackward0>)\n",
      "tensor(0.2776, grad_fn=<DivBackward0>)\n",
      "tensor(0.1187, grad_fn=<DivBackward0>)\n",
      "tensor(0.1515, grad_fn=<DivBackward0>)\n",
      "Epoch number: 4\n",
      "tensor(0.1055, grad_fn=<DivBackward0>)\n",
      "tensor(0.1636, grad_fn=<DivBackward0>)\n",
      "tensor(0.1050, grad_fn=<DivBackward0>)\n",
      "tensor(0.0317, grad_fn=<DivBackward0>)\n",
      "tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "tensor(0.1538, grad_fn=<DivBackward0>)\n",
      "tensor(0.1566, grad_fn=<DivBackward0>)\n",
      "tensor(0.0636, grad_fn=<DivBackward0>)\n",
      "tensor(0.0740, grad_fn=<DivBackward0>)\n",
      "Epoch number: 5\n",
      "tensor(0.0522, grad_fn=<DivBackward0>)\n",
      "tensor(0.1115, grad_fn=<DivBackward0>)\n",
      "tensor(0.0687, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "record_loss = []\n",
    "for i in range(5):\n",
    "    print(f'Epoch number: {i+1}')\n",
    "    j = 0\n",
    "    for sentences, labels in train_dataloader:\n",
    "        output = model(sentences)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 100 == 0:\n",
    "            record_loss.append(loss)\n",
    "            print(loss)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - Knowledge Graphs",
   "language": "python",
   "name": "knowledge-graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
